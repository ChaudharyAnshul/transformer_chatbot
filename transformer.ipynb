{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f4d8b1-ea5b-437c-890e-4a2b2f53ef8f",
   "metadata": {},
   "source": [
    "# Transformer Based Model for Chat-Bot\n",
    "Using Pytorch\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb39cfa-baab-4126-97a5-1b874ad1fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17eecd-c828-44f5-94c0-49e4a963ccf8",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb879d7-e8af-4fce-9efc-64e25aed456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb469f5-8c11-4480-9c63-00a7677dc086",
   "metadata": {},
   "source": [
    "### Set Huperparameters for model\n",
    "\n",
    "- MAX_LEN = 40: The maximum length of input/output sequences (in tokens) for the model.\n",
    "- BATCH_SIZE = 64: The number of training samples processed in one forward/backward pass.\n",
    "- NUM_HEADS = 8: The number of attention heads in the multi-head attention mechanism.\n",
    "- D_MODEL = 512: The dimensionality of the modelâ€™s hidden layer representations.\n",
    "- FFN_UNITS = 2048: The number of units in the feed-forward neural network after each attention layer.\n",
    "- DROPOUT = 0.1: The fraction of units to drop during training to prevent overfitting.\n",
    "- NUM_LAYERS = 4: The number of layers in the encoder and decoder of the Transformer.\n",
    "- EPOCHS = 300: The number of full passes through the training dataset during training.\n",
    "- VOCAB_SIZE = 8000: The number of unique tokens in the model's vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ec4fa34-6cd2-4b58-9010-b4e896e5bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 40\n",
    "BATCH_SIZE = 64\n",
    "NUM_HEADS = 8\n",
    "D_MODEL = 512\n",
    "FFN_UNITS = 2048\n",
    "DROPOUT = 0.1\n",
    "NUM_LAYERS = 4\n",
    "EPOCHS = 300\n",
    "VOCAB_SIZE = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c119b9-6cb3-4d47-b7a3-e2483232fdf5",
   "metadata": {},
   "source": [
    "#### Setup decide for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f3ff5a6-c9cc-4305-8df8-5ce528c35eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ff2fe-40d5-4693-be1f-9f789d3120c8",
   "metadata": {},
   "source": [
    "#### load CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1aa6be1-6ad9-4829-a741-d1f87ed3a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/reminiscences_of_a_stock_operator_qa.csv'\n",
    "data = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b1d25e-3a0c-42d0-be21-024dc61125ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data['question'].tolist()\n",
    "answers = data['answer'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61822ca-dee0-4cfe-b078-261e5bb22143",
   "metadata": {},
   "source": [
    "### Custom Tokenizer to keepp track of vocab and word to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ebb63f-e9df-47df-b438-c25a72969c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word2idx = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.word_count = {}\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                self.word_count[word] = self.word_count.get(word, 0) + 1\n",
    "        sorted_vocab = sorted(self.word_count.items(), key=lambda x: x[1], reverse=True)[:self.vocab_size - 4]\n",
    "        for idx, (word, _) in enumerate(sorted_vocab, start=4):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            seq = [self.word2idx.get(word, self.word2idx[\"<unk>\"]) for word in text.split()]\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for seq in sequences:\n",
    "            text = \" \".join([self.idx2word.get(idx, \"<unk>\") for idx in seq])\n",
    "            texts.append(text)\n",
    "        return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af66fe89-0ec2-4986-8c24-18b4ed6218a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CustomTokenizer(VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(questions + answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb298b-77b0-4a5a-a305-2b0c701bdc65",
   "metadata": {},
   "source": [
    "### Initialize the dataset by converting the questions and answers to sequences of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19561f17-b729-4d70-b4d4-331871c1bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, questions, answers, tokenizer, max_len):\n",
    "        self.questions = tokenizer.texts_to_sequences(questions)\n",
    "        self.answers = tokenizer.texts_to_sequences(answers)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "\n",
    "        question = [1] + question[:self.max_len - 2] + [2]\n",
    "        answer = [1] + answer[:self.max_len - 2] + [2]\n",
    "\n",
    "        question = question + [0] * (self.max_len - len(question))\n",
    "        answer = answer + [0] * (self.max_len - len(answer))\n",
    "\n",
    "        return torch.tensor(question), torch.tensor(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed96643-bc40-433e-a615-37fb540bef3d",
   "metadata": {},
   "source": [
    "### Compute the scaled dot-product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a51001-bcdc-4a28-bb1d-bc09d0bfc215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "    dk = q.size(-1)\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32, device=q.device))\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "    output = torch.matmul(attention_weights, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5225062-e356-4a63-8bb7-ef9e7165c2a1",
   "metadata": {},
   "source": [
    "### Multi Head Atention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7004b140-c5f0-41e4-928f-cd912c579826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        batch_size = q.size(0)\n",
    "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth).transpose(1, 2)\n",
    "\n",
    "        attention = scaled_dot_product_attention(q, k, v, mask)\n",
    "        attention = attention.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.dense(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8dbfe-e06a-4af2-9858-339a50bdc8da",
   "metadata": {},
   "source": [
    "### Initialize the feed-forward network with two linear layers, ReLU activation, and dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b3a10c-87c5-4d09-882c-8b8f35939c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, ffn_units, dropout_rate):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, ffn_units)\n",
    "        self.linear2 = nn.Linear(ffn_units, d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02458d2e-c74a-4c68-b617-9086758ca83a",
   "metadata": {},
   "source": [
    "### Encoder: Multi Head Attention with Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ee82e1c-6f35-4f64-a1a2-b1c6f5a5afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ffn_units, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(d_model, ffn_units, dropout_rate)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        out1 = self.layernorm1(x + self.dropout(attn_output))\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.layernorm2(out1 + self.dropout(ffn_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d2ec75-07ba-4a25-84a6-3188e4a0ded0",
   "metadata": {},
   "source": [
    "### Decoder: With look ahead mask and Cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da4bee1-bfa6-42f0-8cda-27eef2fbca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ffn_units, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.attention1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.attention2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(d_model, ffn_units, dropout_rate)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        attn1 = self.attention1(x, x, x, look_ahead_mask)\n",
    "        out1 = self.layernorm1(x + self.dropout(attn1))\n",
    "\n",
    "        attn2 = self.attention2(out1, enc_output, enc_output, padding_mask)\n",
    "        out2 = self.layernorm2(out1 + self.dropout(attn2))\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        return self.layernorm3(out2 + self.dropout(ffn_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c687f9-584b-452c-ac24-8e18d0df7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c916da2-e8c9-49db-a6f8-4ffea81973cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, ffn_units, num_layers, dropout_rate, max_len):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, ffn_units, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, ffn_units, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "        return mask == 1\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, encoder_mask=None, decoder_mask=None):\n",
    "        # Encoder\n",
    "        encoder_embedded = self.embedding(encoder_input)\n",
    "        encoder_embedded = self.positional_encoding(encoder_embedded)\n",
    "\n",
    "        encoder_output = encoder_embedded\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output, encoder_mask)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_embedded = self.embedding(decoder_input)\n",
    "        decoder_embedded = self.positional_encoding(decoder_embedded)\n",
    "\n",
    "        look_ahead_mask = self.create_look_ahead_mask(decoder_input.size(1)).to(decoder_input.device)\n",
    "\n",
    "        decoder_output = decoder_embedded\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_output, encoder_output, look_ahead_mask, encoder_mask)\n",
    "\n",
    "        return self.fc(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a660e21b-fe89-4415-909d-e46a8bf51b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(VOCAB_SIZE, D_MODEL, NUM_HEADS, FFN_UNITS, NUM_LAYERS, DROPOUT, MAX_LEN)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e824dc40-c911-415e-8666-83dbc4195bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChatDataset(questions, answers, tokenizer, MAX_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8da8b2-9adb-4a14-b7b8-296f0b4c87aa",
   "metadata": {},
   "source": [
    "### Optimizer and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d70f6833-b434-4da7-bcda-47a6245c75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f9ab5a-8f26-422e-8f5c-60d89e46af4e",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "177bc4f8-7e3c-4df0-9bcd-efff2ebf7fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 7.3356, Accuracy: 0.1414\n",
      "Epoch 2/300, Loss: 6.3788, Accuracy: 0.1999\n",
      "Epoch 3/300, Loss: 5.8830, Accuracy: 0.2210\n",
      "Epoch 4/300, Loss: 5.5429, Accuracy: 0.2354\n",
      "Epoch 5/300, Loss: 5.2682, Accuracy: 0.2502\n",
      "Epoch 6/300, Loss: 5.0540, Accuracy: 0.2712\n",
      "Epoch 7/300, Loss: 4.8352, Accuracy: 0.2894\n",
      "Epoch 8/300, Loss: 4.6328, Accuracy: 0.3056\n",
      "Epoch 9/300, Loss: 4.4609, Accuracy: 0.3236\n",
      "Epoch 10/300, Loss: 4.2691, Accuracy: 0.3409\n",
      "Epoch 11/300, Loss: 4.0866, Accuracy: 0.3609\n",
      "Epoch 12/300, Loss: 3.9866, Accuracy: 0.3744\n",
      "Epoch 13/300, Loss: 3.8690, Accuracy: 0.3898\n",
      "Epoch 14/300, Loss: 3.6942, Accuracy: 0.4036\n",
      "Epoch 15/300, Loss: 3.5686, Accuracy: 0.4177\n",
      "Epoch 16/300, Loss: 3.4420, Accuracy: 0.4329\n",
      "Epoch 17/300, Loss: 3.3210, Accuracy: 0.4463\n",
      "Epoch 18/300, Loss: 3.1913, Accuracy: 0.4575\n",
      "Epoch 19/300, Loss: 3.0824, Accuracy: 0.4709\n",
      "Epoch 20/300, Loss: 2.9842, Accuracy: 0.4839\n",
      "Epoch 21/300, Loss: 2.8825, Accuracy: 0.4983\n",
      "Epoch 22/300, Loss: 2.7703, Accuracy: 0.5101\n",
      "Epoch 23/300, Loss: 2.6719, Accuracy: 0.5245\n",
      "Epoch 24/300, Loss: 2.5909, Accuracy: 0.5367\n",
      "Epoch 25/300, Loss: 2.4823, Accuracy: 0.5546\n",
      "Epoch 26/300, Loss: 2.3881, Accuracy: 0.5666\n",
      "Epoch 27/300, Loss: 2.2765, Accuracy: 0.5827\n",
      "Epoch 28/300, Loss: 2.2092, Accuracy: 0.5962\n",
      "Epoch 29/300, Loss: 2.1101, Accuracy: 0.6123\n",
      "Epoch 30/300, Loss: 2.0421, Accuracy: 0.6270\n",
      "Epoch 31/300, Loss: 1.9437, Accuracy: 0.6429\n",
      "Epoch 32/300, Loss: 1.8386, Accuracy: 0.6597\n",
      "Epoch 33/300, Loss: 1.7506, Accuracy: 0.6778\n",
      "Epoch 34/300, Loss: 1.6864, Accuracy: 0.6901\n",
      "Epoch 35/300, Loss: 1.6035, Accuracy: 0.7068\n",
      "Epoch 36/300, Loss: 1.5349, Accuracy: 0.7233\n",
      "Epoch 37/300, Loss: 1.4674, Accuracy: 0.7400\n",
      "Epoch 38/300, Loss: 1.3986, Accuracy: 0.7548\n",
      "Epoch 39/300, Loss: 1.3198, Accuracy: 0.7713\n",
      "Epoch 40/300, Loss: 1.2494, Accuracy: 0.7871\n",
      "Epoch 41/300, Loss: 1.1808, Accuracy: 0.8030\n",
      "Epoch 42/300, Loss: 1.1327, Accuracy: 0.8172\n",
      "Epoch 43/300, Loss: 1.0615, Accuracy: 0.8315\n",
      "Epoch 44/300, Loss: 1.0024, Accuracy: 0.8446\n",
      "Epoch 45/300, Loss: 0.9430, Accuracy: 0.8591\n",
      "Epoch 46/300, Loss: 0.8907, Accuracy: 0.8749\n",
      "Epoch 47/300, Loss: 0.8305, Accuracy: 0.8850\n",
      "Epoch 48/300, Loss: 0.8003, Accuracy: 0.8952\n",
      "Epoch 49/300, Loss: 0.7389, Accuracy: 0.9060\n",
      "Epoch 50/300, Loss: 0.6942, Accuracy: 0.9192\n",
      "Epoch 51/300, Loss: 0.6535, Accuracy: 0.9279\n",
      "Epoch 52/300, Loss: 0.6158, Accuracy: 0.9392\n",
      "Epoch 53/300, Loss: 0.5806, Accuracy: 0.9448\n",
      "Epoch 54/300, Loss: 0.5358, Accuracy: 0.9530\n",
      "Epoch 55/300, Loss: 0.5014, Accuracy: 0.9566\n",
      "Epoch 56/300, Loss: 0.4690, Accuracy: 0.9657\n",
      "Epoch 57/300, Loss: 0.4343, Accuracy: 0.9704\n",
      "Epoch 58/300, Loss: 0.4048, Accuracy: 0.9747\n",
      "Epoch 59/300, Loss: 0.3791, Accuracy: 0.9777\n",
      "Epoch 60/300, Loss: 0.3551, Accuracy: 0.9813\n",
      "Epoch 61/300, Loss: 0.3306, Accuracy: 0.9839\n",
      "Epoch 62/300, Loss: 0.3123, Accuracy: 0.9861\n",
      "Epoch 63/300, Loss: 0.2970, Accuracy: 0.9862\n",
      "Epoch 64/300, Loss: 0.2745, Accuracy: 0.9878\n",
      "Epoch 65/300, Loss: 0.2580, Accuracy: 0.9908\n",
      "Epoch 66/300, Loss: 0.2391, Accuracy: 0.9915\n",
      "Epoch 67/300, Loss: 0.2193, Accuracy: 0.9924\n",
      "Epoch 68/300, Loss: 0.2091, Accuracy: 0.9934\n",
      "Epoch 69/300, Loss: 0.1970, Accuracy: 0.9929\n",
      "Epoch 70/300, Loss: 0.1857, Accuracy: 0.9941\n",
      "Epoch 71/300, Loss: 0.1712, Accuracy: 0.9948\n",
      "Epoch 72/300, Loss: 0.1637, Accuracy: 0.9948\n",
      "Epoch 73/300, Loss: 0.1557, Accuracy: 0.9950\n",
      "Epoch 74/300, Loss: 0.1471, Accuracy: 0.9953\n",
      "Epoch 75/300, Loss: 0.1406, Accuracy: 0.9959\n",
      "Epoch 76/300, Loss: 0.1333, Accuracy: 0.9955\n",
      "Epoch 77/300, Loss: 0.1274, Accuracy: 0.9957\n",
      "Epoch 78/300, Loss: 0.1198, Accuracy: 0.9960\n",
      "Epoch 79/300, Loss: 0.1148, Accuracy: 0.9961\n",
      "Epoch 80/300, Loss: 0.1118, Accuracy: 0.9958\n",
      "Epoch 81/300, Loss: 0.1063, Accuracy: 0.9958\n",
      "Epoch 82/300, Loss: 0.1020, Accuracy: 0.9956\n",
      "Epoch 83/300, Loss: 0.0973, Accuracy: 0.9960\n",
      "Epoch 84/300, Loss: 0.0940, Accuracy: 0.9962\n",
      "Epoch 85/300, Loss: 0.0890, Accuracy: 0.9960\n",
      "Epoch 86/300, Loss: 0.0863, Accuracy: 0.9962\n",
      "Epoch 87/300, Loss: 0.0820, Accuracy: 0.9959\n",
      "Epoch 88/300, Loss: 0.0798, Accuracy: 0.9965\n",
      "Epoch 89/300, Loss: 0.0766, Accuracy: 0.9962\n",
      "Epoch 90/300, Loss: 0.0740, Accuracy: 0.9964\n",
      "Epoch 91/300, Loss: 0.0711, Accuracy: 0.9964\n",
      "Epoch 92/300, Loss: 0.0690, Accuracy: 0.9963\n",
      "Epoch 93/300, Loss: 0.0673, Accuracy: 0.9959\n",
      "Epoch 94/300, Loss: 0.0652, Accuracy: 0.9960\n",
      "Epoch 95/300, Loss: 0.0636, Accuracy: 0.9965\n",
      "Epoch 96/300, Loss: 0.0627, Accuracy: 0.9963\n",
      "Epoch 97/300, Loss: 0.0609, Accuracy: 0.9960\n",
      "Epoch 98/300, Loss: 0.0590, Accuracy: 0.9963\n",
      "Epoch 99/300, Loss: 0.0562, Accuracy: 0.9966\n",
      "Epoch 100/300, Loss: 0.0550, Accuracy: 0.9963\n",
      "Epoch 101/300, Loss: 0.0534, Accuracy: 0.9964\n",
      "Epoch 102/300, Loss: 0.0517, Accuracy: 0.9964\n",
      "Epoch 103/300, Loss: 0.0505, Accuracy: 0.9965\n",
      "Epoch 104/300, Loss: 0.0490, Accuracy: 0.9968\n",
      "Epoch 105/300, Loss: 0.0478, Accuracy: 0.9967\n",
      "Epoch 106/300, Loss: 0.0466, Accuracy: 0.9968\n",
      "Epoch 107/300, Loss: 0.0458, Accuracy: 0.9967\n",
      "Epoch 108/300, Loss: 0.0448, Accuracy: 0.9968\n",
      "Epoch 109/300, Loss: 0.0433, Accuracy: 0.9967\n",
      "Epoch 110/300, Loss: 0.0420, Accuracy: 0.9966\n",
      "Epoch 111/300, Loss: 0.0415, Accuracy: 0.9965\n",
      "Epoch 112/300, Loss: 0.0408, Accuracy: 0.9966\n",
      "Epoch 113/300, Loss: 0.0401, Accuracy: 0.9968\n",
      "Epoch 114/300, Loss: 0.0396, Accuracy: 0.9968\n",
      "Epoch 115/300, Loss: 0.0385, Accuracy: 0.9969\n",
      "Epoch 116/300, Loss: 0.0383, Accuracy: 0.9965\n",
      "Epoch 117/300, Loss: 0.0364, Accuracy: 0.9968\n",
      "Epoch 118/300, Loss: 0.0357, Accuracy: 0.9965\n",
      "Epoch 119/300, Loss: 0.0355, Accuracy: 0.9966\n",
      "Epoch 120/300, Loss: 0.0347, Accuracy: 0.9964\n",
      "Epoch 121/300, Loss: 0.0338, Accuracy: 0.9971\n",
      "Epoch 122/300, Loss: 0.0345, Accuracy: 0.9965\n",
      "Epoch 123/300, Loss: 0.0326, Accuracy: 0.9968\n",
      "Epoch 124/300, Loss: 0.0319, Accuracy: 0.9967\n",
      "Epoch 125/300, Loss: 0.0313, Accuracy: 0.9969\n",
      "Epoch 126/300, Loss: 0.0309, Accuracy: 0.9963\n",
      "Epoch 127/300, Loss: 0.0310, Accuracy: 0.9966\n",
      "Epoch 128/300, Loss: 0.0308, Accuracy: 0.9966\n",
      "Epoch 129/300, Loss: 0.0317, Accuracy: 0.9967\n",
      "Epoch 130/300, Loss: 0.0313, Accuracy: 0.9965\n",
      "Epoch 131/300, Loss: 0.0302, Accuracy: 0.9963\n",
      "Epoch 132/300, Loss: 0.0297, Accuracy: 0.9966\n",
      "Epoch 133/300, Loss: 0.0314, Accuracy: 0.9959\n",
      "Epoch 134/300, Loss: 0.0329, Accuracy: 0.9959\n",
      "Epoch 135/300, Loss: 0.0305, Accuracy: 0.9962\n",
      "Epoch 136/300, Loss: 0.0288, Accuracy: 0.9967\n",
      "Epoch 137/300, Loss: 0.0308, Accuracy: 0.9959\n",
      "Epoch 138/300, Loss: 0.0295, Accuracy: 0.9966\n",
      "Epoch 139/300, Loss: 0.0287, Accuracy: 0.9968\n",
      "Epoch 140/300, Loss: 0.0279, Accuracy: 0.9962\n",
      "Epoch 141/300, Loss: 0.0267, Accuracy: 0.9966\n",
      "Epoch 142/300, Loss: 0.0261, Accuracy: 0.9968\n",
      "Epoch 143/300, Loss: 0.0248, Accuracy: 0.9966\n",
      "Epoch 144/300, Loss: 0.0246, Accuracy: 0.9967\n",
      "Epoch 145/300, Loss: 0.0252, Accuracy: 0.9965\n",
      "Epoch 146/300, Loss: 0.0258, Accuracy: 0.9967\n",
      "Epoch 147/300, Loss: 0.0252, Accuracy: 0.9965\n",
      "Epoch 148/300, Loss: 0.0238, Accuracy: 0.9963\n",
      "Epoch 149/300, Loss: 0.0230, Accuracy: 0.9968\n",
      "Epoch 150/300, Loss: 0.0222, Accuracy: 0.9970\n",
      "Epoch 151/300, Loss: 0.0221, Accuracy: 0.9970\n",
      "Epoch 152/300, Loss: 0.0220, Accuracy: 0.9967\n",
      "Epoch 153/300, Loss: 0.0221, Accuracy: 0.9968\n",
      "Epoch 154/300, Loss: 0.0213, Accuracy: 0.9966\n",
      "Epoch 155/300, Loss: 0.0205, Accuracy: 0.9965\n",
      "Epoch 156/300, Loss: 0.0204, Accuracy: 0.9968\n",
      "Epoch 157/300, Loss: 0.0204, Accuracy: 0.9965\n",
      "Epoch 158/300, Loss: 0.0209, Accuracy: 0.9968\n",
      "Epoch 159/300, Loss: 0.0204, Accuracy: 0.9965\n",
      "Epoch 160/300, Loss: 0.0205, Accuracy: 0.9966\n",
      "Epoch 161/300, Loss: 0.0204, Accuracy: 0.9965\n",
      "Epoch 162/300, Loss: 0.0202, Accuracy: 0.9964\n",
      "Epoch 163/300, Loss: 0.0186, Accuracy: 0.9970\n",
      "Epoch 164/300, Loss: 0.0187, Accuracy: 0.9966\n",
      "Epoch 165/300, Loss: 0.0187, Accuracy: 0.9969\n",
      "Epoch 166/300, Loss: 0.0186, Accuracy: 0.9968\n",
      "Epoch 167/300, Loss: 0.0180, Accuracy: 0.9967\n",
      "Epoch 168/300, Loss: 0.0184, Accuracy: 0.9965\n",
      "Epoch 169/300, Loss: 0.0181, Accuracy: 0.9964\n",
      "Epoch 170/300, Loss: 0.0169, Accuracy: 0.9966\n",
      "Epoch 171/300, Loss: 0.0168, Accuracy: 0.9965\n",
      "Epoch 172/300, Loss: 0.0171, Accuracy: 0.9963\n",
      "Epoch 173/300, Loss: 0.0171, Accuracy: 0.9969\n",
      "Epoch 174/300, Loss: 0.0164, Accuracy: 0.9967\n",
      "Epoch 175/300, Loss: 0.0173, Accuracy: 0.9965\n",
      "Epoch 176/300, Loss: 0.0178, Accuracy: 0.9964\n",
      "Epoch 177/300, Loss: 0.0176, Accuracy: 0.9967\n",
      "Epoch 178/300, Loss: 0.0177, Accuracy: 0.9966\n",
      "Epoch 179/300, Loss: 0.0172, Accuracy: 0.9965\n",
      "Epoch 180/300, Loss: 0.0171, Accuracy: 0.9967\n",
      "Epoch 181/300, Loss: 0.0176, Accuracy: 0.9964\n",
      "Epoch 182/300, Loss: 0.0176, Accuracy: 0.9967\n",
      "Epoch 183/300, Loss: 0.0166, Accuracy: 0.9964\n",
      "Epoch 184/300, Loss: 0.0166, Accuracy: 0.9962\n",
      "Epoch 185/300, Loss: 0.0159, Accuracy: 0.9968\n",
      "Epoch 186/300, Loss: 0.0159, Accuracy: 0.9963\n",
      "Epoch 187/300, Loss: 0.0171, Accuracy: 0.9964\n",
      "Epoch 188/300, Loss: 0.0157, Accuracy: 0.9968\n",
      "Epoch 189/300, Loss: 0.0157, Accuracy: 0.9968\n",
      "Epoch 190/300, Loss: 0.0156, Accuracy: 0.9969\n",
      "Epoch 191/300, Loss: 0.0153, Accuracy: 0.9968\n",
      "Epoch 192/300, Loss: 0.0147, Accuracy: 0.9964\n",
      "Epoch 193/300, Loss: 0.0147, Accuracy: 0.9965\n",
      "Epoch 194/300, Loss: 0.0148, Accuracy: 0.9963\n",
      "Epoch 195/300, Loss: 0.0148, Accuracy: 0.9962\n",
      "Epoch 196/300, Loss: 0.0141, Accuracy: 0.9968\n",
      "Epoch 197/300, Loss: 0.0142, Accuracy: 0.9968\n",
      "Epoch 198/300, Loss: 0.0157, Accuracy: 0.9964\n",
      "Epoch 199/300, Loss: 0.0151, Accuracy: 0.9964\n",
      "Epoch 200/300, Loss: 0.0153, Accuracy: 0.9966\n",
      "Epoch 201/300, Loss: 0.0187, Accuracy: 0.9963\n",
      "Epoch 202/300, Loss: 0.0205, Accuracy: 0.9956\n",
      "Epoch 203/300, Loss: 0.0228, Accuracy: 0.9949\n",
      "Epoch 204/300, Loss: 0.0256, Accuracy: 0.9951\n",
      "Epoch 205/300, Loss: 0.0288, Accuracy: 0.9941\n",
      "Epoch 206/300, Loss: 0.0288, Accuracy: 0.9947\n",
      "Epoch 207/300, Loss: 0.0317, Accuracy: 0.9943\n",
      "Epoch 208/300, Loss: 0.0282, Accuracy: 0.9949\n",
      "Epoch 209/300, Loss: 0.0245, Accuracy: 0.9959\n",
      "Epoch 210/300, Loss: 0.0245, Accuracy: 0.9955\n",
      "Epoch 211/300, Loss: 0.0243, Accuracy: 0.9952\n",
      "Epoch 212/300, Loss: 0.0205, Accuracy: 0.9956\n",
      "Epoch 213/300, Loss: 0.0182, Accuracy: 0.9962\n",
      "Epoch 214/300, Loss: 0.0157, Accuracy: 0.9964\n",
      "Epoch 215/300, Loss: 0.0148, Accuracy: 0.9964\n",
      "Epoch 216/300, Loss: 0.0143, Accuracy: 0.9967\n",
      "Epoch 217/300, Loss: 0.0152, Accuracy: 0.9967\n",
      "Epoch 218/300, Loss: 0.0145, Accuracy: 0.9967\n",
      "Epoch 219/300, Loss: 0.0144, Accuracy: 0.9966\n",
      "Epoch 220/300, Loss: 0.0135, Accuracy: 0.9966\n",
      "Epoch 221/300, Loss: 0.0134, Accuracy: 0.9966\n",
      "Epoch 222/300, Loss: 0.0130, Accuracy: 0.9968\n",
      "Epoch 223/300, Loss: 0.0125, Accuracy: 0.9967\n",
      "Epoch 224/300, Loss: 0.0121, Accuracy: 0.9969\n",
      "Epoch 225/300, Loss: 0.0131, Accuracy: 0.9962\n",
      "Epoch 226/300, Loss: 0.0126, Accuracy: 0.9968\n",
      "Epoch 227/300, Loss: 0.0122, Accuracy: 0.9968\n",
      "Epoch 228/300, Loss: 0.0122, Accuracy: 0.9968\n",
      "Epoch 229/300, Loss: 0.0122, Accuracy: 0.9969\n",
      "Epoch 230/300, Loss: 0.0118, Accuracy: 0.9966\n",
      "Epoch 231/300, Loss: 0.0118, Accuracy: 0.9967\n",
      "Epoch 232/300, Loss: 0.0119, Accuracy: 0.9967\n",
      "Epoch 233/300, Loss: 0.0123, Accuracy: 0.9965\n",
      "Epoch 234/300, Loss: 0.0120, Accuracy: 0.9967\n",
      "Epoch 235/300, Loss: 0.0110, Accuracy: 0.9965\n",
      "Epoch 236/300, Loss: 0.0116, Accuracy: 0.9962\n",
      "Epoch 237/300, Loss: 0.0107, Accuracy: 0.9967\n",
      "Epoch 238/300, Loss: 0.0122, Accuracy: 0.9965\n",
      "Epoch 239/300, Loss: 0.0107, Accuracy: 0.9967\n",
      "Epoch 240/300, Loss: 0.0115, Accuracy: 0.9966\n",
      "Epoch 241/300, Loss: 0.0109, Accuracy: 0.9970\n",
      "Epoch 242/300, Loss: 0.0119, Accuracy: 0.9968\n",
      "Epoch 243/300, Loss: 0.0116, Accuracy: 0.9965\n",
      "Epoch 244/300, Loss: 0.0116, Accuracy: 0.9962\n",
      "Epoch 245/300, Loss: 0.0116, Accuracy: 0.9967\n",
      "Epoch 246/300, Loss: 0.0113, Accuracy: 0.9969\n",
      "Epoch 247/300, Loss: 0.0107, Accuracy: 0.9969\n",
      "Epoch 248/300, Loss: 0.0103, Accuracy: 0.9968\n",
      "Epoch 249/300, Loss: 0.0109, Accuracy: 0.9965\n",
      "Epoch 250/300, Loss: 0.0101, Accuracy: 0.9968\n",
      "Epoch 251/300, Loss: 0.0104, Accuracy: 0.9962\n",
      "Epoch 252/300, Loss: 0.0101, Accuracy: 0.9965\n",
      "Epoch 253/300, Loss: 0.0107, Accuracy: 0.9965\n",
      "Epoch 254/300, Loss: 0.0104, Accuracy: 0.9967\n",
      "Epoch 255/300, Loss: 0.0111, Accuracy: 0.9966\n",
      "Epoch 256/300, Loss: 0.0103, Accuracy: 0.9965\n",
      "Epoch 257/300, Loss: 0.0117, Accuracy: 0.9965\n",
      "Epoch 258/300, Loss: 0.0112, Accuracy: 0.9965\n",
      "Epoch 259/300, Loss: 0.0125, Accuracy: 0.9964\n",
      "Epoch 260/300, Loss: 0.0144, Accuracy: 0.9955\n",
      "Epoch 261/300, Loss: 0.0125, Accuracy: 0.9961\n",
      "Epoch 262/300, Loss: 0.0113, Accuracy: 0.9961\n",
      "Epoch 263/300, Loss: 0.0122, Accuracy: 0.9963\n",
      "Epoch 264/300, Loss: 0.0112, Accuracy: 0.9965\n",
      "Epoch 265/300, Loss: 0.0109, Accuracy: 0.9966\n",
      "Epoch 266/300, Loss: 0.0110, Accuracy: 0.9968\n",
      "Epoch 267/300, Loss: 0.0112, Accuracy: 0.9968\n",
      "Epoch 268/300, Loss: 0.0115, Accuracy: 0.9967\n",
      "Epoch 269/300, Loss: 0.0108, Accuracy: 0.9964\n",
      "Epoch 270/300, Loss: 0.0113, Accuracy: 0.9963\n",
      "Epoch 271/300, Loss: 0.0117, Accuracy: 0.9963\n",
      "Epoch 272/300, Loss: 0.0104, Accuracy: 0.9968\n",
      "Epoch 273/300, Loss: 0.0105, Accuracy: 0.9962\n",
      "Epoch 274/300, Loss: 0.0107, Accuracy: 0.9969\n",
      "Epoch 275/300, Loss: 0.0102, Accuracy: 0.9965\n",
      "Epoch 276/300, Loss: 0.0103, Accuracy: 0.9967\n",
      "Epoch 277/300, Loss: 0.0095, Accuracy: 0.9969\n",
      "Epoch 278/300, Loss: 0.0099, Accuracy: 0.9965\n",
      "Epoch 279/300, Loss: 0.0098, Accuracy: 0.9963\n",
      "Epoch 280/300, Loss: 0.0124, Accuracy: 0.9964\n",
      "Epoch 281/300, Loss: 0.0112, Accuracy: 0.9966\n",
      "Epoch 282/300, Loss: 0.0100, Accuracy: 0.9967\n",
      "Epoch 283/300, Loss: 0.0101, Accuracy: 0.9965\n",
      "Epoch 284/300, Loss: 0.0100, Accuracy: 0.9965\n",
      "Epoch 285/300, Loss: 0.0095, Accuracy: 0.9968\n",
      "Epoch 286/300, Loss: 0.0097, Accuracy: 0.9967\n",
      "Epoch 287/300, Loss: 0.0097, Accuracy: 0.9966\n",
      "Epoch 288/300, Loss: 0.0115, Accuracy: 0.9965\n",
      "Epoch 289/300, Loss: 0.0110, Accuracy: 0.9965\n",
      "Epoch 290/300, Loss: 0.0108, Accuracy: 0.9965\n",
      "Epoch 291/300, Loss: 0.0110, Accuracy: 0.9969\n",
      "Epoch 292/300, Loss: 0.0102, Accuracy: 0.9968\n",
      "Epoch 293/300, Loss: 0.0100, Accuracy: 0.9965\n",
      "Epoch 294/300, Loss: 0.0095, Accuracy: 0.9968\n",
      "Epoch 295/300, Loss: 0.0096, Accuracy: 0.9966\n",
      "Epoch 296/300, Loss: 0.0106, Accuracy: 0.9965\n",
      "Epoch 297/300, Loss: 0.0101, Accuracy: 0.9965\n",
      "Epoch 298/300, Loss: 0.0105, Accuracy: 0.9966\n",
      "Epoch 299/300, Loss: 0.0122, Accuracy: 0.9963\n",
      "Epoch 300/300, Loss: 0.0138, Accuracy: 0.9960\n"
     ]
    }
   ],
   "source": [
    "# Training Loop with Loss and Accuracy\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for questions_batch, answers_batch in dataloader:\n",
    "        questions_batch = questions_batch.to(device)\n",
    "        decoder_input = answers_batch[:, :-1].to(device)  # Input for decoder\n",
    "        target = answers_batch[:, 1:].to(device)  # Target for training\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(questions_batch, decoder_input)\n",
    "        loss = criterion(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted_tokens = torch.argmax(output, dim=-1)\n",
    "        correct_tokens = (predicted_tokens == target).sum().item()\n",
    "        total_accuracy += correct_tokens\n",
    "        total_tokens += target.numel()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_accuracy = total_accuracy / total_tokens\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b389d-7ea7-419f-ae58-98dff048e88f",
   "metadata": {},
   "source": [
    "### Export the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62ea317a-2071-404d-ac52-736e81aa201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/transformer_chatbot_gpu_deco_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655b734-09b0-49bc-a150-24fdeb13b25b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f7c7476-7b2d-4e0c-892c-ae0dbfda7c01",
   "metadata": {},
   "source": [
    "### Load and Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3a732d0-3507-426d-8e02-ef7e95b2e0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what was your first job in finance?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: My first job was as a quotation-board boy at a stock brokerage firm. I was quick with numbers and excelled at mental arithmetic, skills that proved invaluable later in my career.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what prompted you to leave bucket shop?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The bucket shops became unhappy with my consistent success, and they began to actively restrict my trades. It was time to seek a more challenging and professional trading environment.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  why did you to leave bucket shop?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The bucket shops became unhappy with my consistent success, and they began to actively restrict my trades. It was time to seek a more challenging and professional trading environment.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  why did you leave bucket shop?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The bucket shops became unhappy with my consistent success, and they began to actively restrict my trades. It was time to seek a more challenging and professional trading environment.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how did you understanding of treands influence your success?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: My understanding of market psychology played a critical role in the influence of the influence of fear and greed from their own judgments and greed from their mistakes. Recognizing the influence of others.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what did you learn from your biggest loss?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: My biggest loss reinforced the importance of anticipating market shifts and managing risk effectively. It also highlighted the need for maintaining emotional control and a consistent approach to trading.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Load the Trained Model\n",
    "model.load_state_dict(torch.load(\"model/transformer_chatbot_gpu_deco_1.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Chat Function\n",
    "def chat_response(question, tokenizer, model, max_len=40):\n",
    "    model.eval()\n",
    "    question_seq = tokenizer.texts_to_sequences([question])[0]\n",
    "    question_seq = [1] + question_seq[:max_len - 2] + [2]  # Add <start> and <end> tokens\n",
    "    question_seq = question_seq + [0] * (max_len - len(question_seq))  # Pad to max_len\n",
    "    question_tensor = torch.tensor([question_seq]).to(device)\n",
    "\n",
    "    decoder_input = torch.tensor([[1]]).to(device)  # Start token\n",
    "    response = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(question_tensor, decoder_input)\n",
    "        predicted_id = torch.argmax(output[:, -1, :], dim=-1).item()\n",
    "        if predicted_id == 2:  # End token\n",
    "            break\n",
    "        response.append(predicted_id)\n",
    "        decoder_input = torch.cat([decoder_input, torch.tensor([[predicted_id]]).to(device)], dim=-1)\n",
    "\n",
    "    return tokenizer.sequences_to_texts([response])[0].replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
    "\n",
    "# Interactive Chat Loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting chatbot. Goodbye!\")\n",
    "        break\n",
    "    bot_response = chat_response(user_input, tokenizer, model)\n",
    "    print(f\"Bot: {bot_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c75c2-47dd-4313-b4ab-3173781bf9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
